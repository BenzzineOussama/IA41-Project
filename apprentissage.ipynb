{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from force3env1 import Force3Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "MINIBATCH_SIZE = 64\n",
    "TAU = 1e-3\n",
    "E_DECAY = 0.99\n",
    "E_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor(\n",
    "        np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    next_states = tf.convert_to_tensor(\n",
    "        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_vals = tf.convert_to_tensor(\n",
    "        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    return (states, actions, rewards, next_states, done_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, epsilon * E_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network(q_network, target_q_network):\n",
    "    for target_weights, q_net_weights in zip(\n",
    "        target_q_network.weights, q_network.weights\n",
    "    ):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_update_conditions(t, num_steps_upd, memory_buffer):\n",
    "    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez la graine aléatoire pour TensorFlow\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "ALPHA = 1e-3\n",
    "MEMORY_SIZE = 100_000     \n",
    "GAMMA = 0.99               \n",
    "NUM_STEPS_FOR_UPDATE = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (9,)\n",
      "Number of actions: 251\n"
     ]
    }
   ],
   "source": [
    "env = Force3Env()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = len(env.valid_actions)\n",
    "\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=state_size),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(num_actions, activation='linear')\n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=state_size),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(num_actions, activation='linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "    action_types, start_pos, target_pos, _ = tf.split(actions, num_or_size_splits=4, axis=1)\n",
    "    \n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    y_targets = rewards + (1 - done_vals) * gamma * max_qsa\n",
    "\n",
    "    # Convertir les composants d'action en indices uniques\n",
    "    N = 9  # Nombre de valeurs possibles pour start_pos et target_pos\n",
    "    # Convertir les composants d'action en indices uniques\n",
    "    # Pour les actions de type 0 (Placer), les indices vont de 0 à 8\n",
    "    # Pour les actions de type 1 (Déplacer un pion rond), les indices vont de 9 à 89\n",
    "    # Pour les actions de type 2 (Déplacer un carré), les indices vont de 90 à 251\n",
    "    action_indices = action_types * (N**2 + N**2) + start_pos * N + target_pos\n",
    "    # Pour les actions de type 0, les indices vont de 0 à 8\n",
    "    action_indices = tf.where(action_types == 0, target_pos, action_types)\n",
    "    # Pour les actions de type 1, les indices vont de 9 à 89 (9 + 9 * 9)\n",
    "    action_indices = tf.where(action_types == 1, 9 + start_pos * N + target_pos, action_indices)\n",
    "    # Pour les actions de type 2, les indices vont de 90 à 251 (90 + 9 * 9 * 2)\n",
    "    action_indices = tf.where(action_types == 2, 90 + start_pos * N + target_pos * 2, action_indices)\n",
    "    action_indices = tf.squeeze(action_indices, axis=-1)  # Pour enlever une dimension inutile\n",
    "\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), \n",
    "                                                tf.cast(action_indices, tf.int32)], axis=1))\n",
    "        \n",
    "    loss = MSE(q_values, y_targets)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_values, epsilon=0.0):\n",
    "    if random.random() > epsilon:\n",
    "        # Exploiter: choisir la meilleure action basée sur les valeurs Q prédites\n",
    "        return env.valid_actions[np.argmax(q_values.numpy())]\n",
    "    else:\n",
    "        # Explorer: choisir une action au hasard\n",
    "        return random.choice(env.valid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -1792.80\n",
      "Episode 200 | Total point average of the last 100 episodes: -1766.40\n",
      "Episode 300 | Total point average of the last 100 episodes: -1776.40\n",
      "Episode 400 | Total point average of the last 100 episodes: -1897.00\n",
      "Episode 500 | Total point average of the last 100 episodes: -1940.80\n",
      "Episode 600 | Total point average of the last 100 episodes: -1942.60\n",
      "Episode 700 | Total point average of the last 100 episodes: -1940.00\n",
      "Episode 800 | Total point average of the last 100 episodes: -1938.40\n",
      "Episode 900 | Total point average of the last 100 episodes: -1938.40\n",
      "Episode 1000 | Total point average of the last 100 episodes: -1938.40\n",
      "\n",
      "Total Runtime: 246.49 s (4.11 min)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_episodes = 1000\n",
    "max_num_timesteps = 100\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    \n",
    "epsilon = 1.0     \n",
    "\n",
    "\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        \n",
    "        state_qn = np.expand_dims(state, axis=0)  \n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values, epsilon)\n",
    "        \n",
    "       \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            \n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            \n",
    "            \n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    \n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('force3.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
